### Apache Airflow â€“ ETL Pipeline

This folder contains the Apache Airflow implementation of the ETL pipeline used in the Comparative Data Orchestration project.

The goal of this pipeline is to demonstrate how the same ETL logic can be orchestrated using Airflow and compared with other tools like Prefect and Dagster.

ğŸ“Œ Overview

The Airflow pipeline performs the following steps:

Extract

Reads event data from a CSV file (synthetic_events.csv)

Transform

Filters out blocked countries

Computes:

Event count per user per day

Session duration per user

Load

Writes the final result as Parquet files

Data is partitioned by date

Each step is implemented as a separate Airflow task.

ğŸ“ Folder Structure
airflow/
â”‚
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ etl_airflow.py
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ synthetic_events.csv
â”‚   â””â”€â”€ output_airflow/
â”‚       â””â”€â”€ date=YYYY-MM-DD/
â”‚
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ README.md

âš™ï¸ Technologies Used

Apache Airflow 2.8.1

Docker & Docker Compose

PostgreSQL (Airflow metadata database)

Pandas

PyArrow (Parquet output)

â–¶ï¸ How to Run the Airflow Pipeline
1ï¸âƒ£ Start Airflow Services

From the airflow/ directory:

docker compose up -d


This starts:

PostgreSQL

Airflow Webserver

Airflow Scheduler

2ï¸âƒ£ Access Airflow UI

Open your browser and go to:

http://localhost:8080


Login credentials (if required):

Username: admin
Password: admin

3ï¸âƒ£ Enable the DAG

Locate the DAG named etl_airflow

Turn the toggle ON

Trigger the DAG manually or let it run on schedule

ğŸ” Backfill Execution

Airflow supports running pipelines for historical dates.

Example backfill command:

airflow dags backfill etl_airflow -s 2024-01-01 -e 2024-01-03


This executes the pipeline for past dates and generates corresponding partitions.

ğŸ“¤ Output

The output is written to:

airflow/data/output_airflow/


Partitioned by date:

output_airflow/
â”œâ”€â”€ date=2024-01-01/
â”œâ”€â”€ date=2024-01-02/
â””â”€â”€ date=2024-01-03/


Each partition contains Parquet files with identical schema to the Prefect and Dagster outputs.

ğŸ§  Key Learnings

Airflow requires explicit DAG and task definitions

Scheduler and webserver must both be running

Backfill is a strong feature for historical reprocessing

Setup is more complex compared to Prefect, but very powerful for production workflows

âœ… Status

âœ” Pipeline runs successfully
âœ” Output generated in Parquet format
âœ” Used for comparison with Prefect and Dagster